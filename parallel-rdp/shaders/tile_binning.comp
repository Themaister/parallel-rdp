#version 450
/* Copyright (c) 2020 Themaister
 *
 * Permission is hereby granted, free of charge, to any person obtaining
 * a copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sublicense, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice shall be
 * included in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 */
// Consumes result from tile_binning_prepass.comp, bins at a finer resolution (8x8 or 16x16 blocks).
#include "small_types.h"

// Implementation architecture from RetroWarp.

#if SUBGROUP
#extension GL_KHR_shader_subgroup_basic : require
#extension GL_KHR_shader_subgroup_vote : require
#extension GL_KHR_shader_subgroup_ballot : require
#extension GL_KHR_shader_subgroup_arithmetic : require
layout(local_size_x_id = 0) in;
#else
// Reasonable default. For AMD (64 threads), subgroups are definitely supported, so this won't be hit.
layout(local_size_x = 32) in;
#endif

#include "debug.h"
#include "data_structures.h"
#include "binning.h"

layout(constant_id = 1) const int TILE_WIDTH = 8;
layout(constant_id = 2) const int TILE_HEIGHT = 8;
layout(constant_id = 3) const int TILE_DOWNSAMPLE_LOG2 = 2;
layout(constant_id = 4) const int MAX_PRIMITIVES = 256;
layout(constant_id = 5) const int MAX_WIDTH = 1024;
layout(constant_id = 6) const int TILE_INSTANCE_STRIDE = 0x10000;

const int TILE_BINNING_STRIDE = MAX_PRIMITIVES / 32;
const int MAX_TILES_X = MAX_WIDTH / TILE_WIDTH;
const int MAX_TILES_X_LOW_RES = MAX_WIDTH / (TILE_WIDTH << TILE_DOWNSAMPLE_LOG2);

layout(set = 0, binding = 0, std430) readonly buffer TriangleSetupBuffer
{
    TriangleSetupMem elems[];
} triangle_setup;
#include "load_triangle_setup.h"

layout(set = 0, binding = 1, std430) readonly buffer ScissorStateBuffer
{
    ScissorStateMem elems[];
} scissor_state;
#include "load_scissor_state.h"

layout(set = 0, binding = 2, std430) readonly buffer StateIndicesBuffer
{
    InstanceIndicesMem elems[];
} state_indices;

layout(std430, set = 0, binding = 3) writeonly buffer TileBitmask
{
    uint binned_bitmask[];
};

layout(std430, set = 0, binding = 4) readonly buffer TileBitmaskLowRes
{
    uint binned_bitmask_low_res[];
};

layout(std430, set = 0, binding = 5) writeonly buffer TileBitmaskCoarse
{
    uint binned_bitmask_coarse[];
};

#if !UBERSHADER
layout(std430, set = 0, binding = 6) writeonly buffer TileInstanceOffset
{
    uint elems[];
} tile_instance_offsets;

layout(std430, set = 0, binding = 7) buffer IndirectBuffer
{
    uvec4 elems[];
} indirect_counts;

// This can actually be uint16_t, but AMD doesn't seem to support loading uint16_t in SMEM unit,
// the memory traffic for this data structure is not relevant anyways.
struct TileRasterWork
{
    uint tile_x, tile_y;
    uint tile_instance;
    uint primitive;
};

layout(std430, set = 0, binding = 8) writeonly buffer WorkList
{
    uvec4 elems[];
} tile_raster_work;
#endif

#if !SUBGROUP
shared uint merged_mask;
#endif

#if !UBERSHADER
uint allocate_work_offset(uint variant_index)
{
#if !SUBGROUP
    return atomicAdd(indirect_counts.elems[variant_index].x, 1u);
#else
    // Merge atomic operations.
    // XXX: This is kinda sketchy, and almost all shader compilers fail on a slightly different implementation of this loop.
    bool work_to_do = true;
    uint res;
    do
    {
        if (subgroupBroadcastFirst(variant_index) == variant_index)
        {
            uvec4 active_mask = subgroupBallot(true);
            uint count = subgroupBallotBitCount(active_mask);
            uint work_offset = 0u;
            if (subgroupElect())
                work_offset = atomicAdd(indirect_counts.elems[variant_index].x, count);

            work_offset = subgroupBroadcastFirst(work_offset);
            work_offset += subgroupBallotExclusiveBitCount(active_mask);
            res = work_offset;
            work_to_do = false;
        }
    } while(work_to_do);
    return res;
#endif
}
#endif

layout(push_constant, std430) uniform Registers
{
    uvec2 resolution;
    uint primitive_count;
    uint primitive_count_32;
} fb_info;

void main()
{
    ivec2 tile = ivec2(gl_WorkGroupID.yz);
    ivec2 base_coord = tile * ivec2(TILE_WIDTH, TILE_HEIGHT);
    ivec2 end_coord = min(base_coord + ivec2(TILE_WIDTH, TILE_HEIGHT), ivec2(fb_info.resolution)) - 1;

    int linear_tile = tile.y * MAX_TILES_X + tile.x;
#if SUBGROUP
    // Spec is unclear how gl_LocalInvocationIndex is mapped to gl_SubgroupInvocationID, so synthesize our own.
    // We know the subgroups are fully occupied with VK_EXT_subgroup_size_control already.
    uint local_index = gl_SubgroupInvocationID;
    int mask_index = int(gl_SubgroupInvocationID + gl_SubgroupID * gl_SubgroupSize + gl_WorkGroupID.x * gl_WorkGroupSize.x);
#else
    uint local_index = gl_LocalInvocationIndex;
    int mask_index = int(gl_GlobalInvocationID.x);
#endif

    bool group_bin_to_tile = false;
    uint binned = 0u;
    if (mask_index < fb_info.primitive_count_32)
    {
        int linear_tile_lowres = (tile.y >> TILE_DOWNSAMPLE_LOG2) * MAX_TILES_X_LOW_RES + (tile.x >> TILE_DOWNSAMPLE_LOG2);
        int binned_bitmask_offset = linear_tile_lowres * TILE_BINNING_STRIDE + mask_index;

        // Each threads works on 32 primitives at once. Most likely, we'll only loop a few times here
        // due to low-res prepass binning having completed before.
        uint low_res_binned = binned_bitmask_low_res[binned_bitmask_offset];
        while (low_res_binned != 0u)
        {
            int i = findLSB(low_res_binned);
            low_res_binned &= ~uint(1 << i);

            int primitive_index = i + mask_index * 32;

            ScissorState scissor = load_scissor_state(primitive_index);
            ivec2 clipped_base_coord = max(base_coord, ivec2(scissor.xlo, scissor.ylo) >> 2);
            ivec2 clipped_end_coord = min(end_coord, ivec2(scissor.xhi, scissor.yhi) >> 2);
            TriangleSetup setup = load_triangle_setup(primitive_index);

            if (bin_primitive(setup, clipped_base_coord, clipped_end_coord))
                binned |= 1u << uint(i);
        }

        binned_bitmask[linear_tile * TILE_BINNING_STRIDE + mask_index] = binned;
        group_bin_to_tile = binned != 0u;
    }

    // Now, we reduce the group_bin_to_tile to a single u32 bitmask which is used as the highest level
    // bitmap which we can loop over.

#if SUBGROUP
    uvec4 ballot_result = subgroupBallot(group_bin_to_tile);

#if !UBERSHADER
    uint bit_count = uint(bitCount(binned));
    uint instance_offset = 0u;
    if (subgroupAny(bit_count != 0u))
    {
        // Allocate tile instance space for all threads in subgroup in one go.
        uint total_bit_count = subgroupAdd(bit_count);

        if (subgroupElect())
            if (total_bit_count != 0u)
                instance_offset = atomicAdd(indirect_counts.elems[0].w, total_bit_count);

        instance_offset = subgroupBroadcastFirst(instance_offset);
        instance_offset += subgroupInclusiveAdd(bit_count) - bit_count;
    }
#endif

    if (subgroupElect())
    {
        uint binned_bitmask_offset = uint(linear_tile);
        // gl_SubgroupSize of 128 is a theoretical thing, but no GPU does that ...
        if (gl_SubgroupSize == 64u)
        {
            binned_bitmask_coarse[binned_bitmask_offset + 2u * gl_WorkGroupID.x] = ballot_result.x;
            //binned_bitmask_coarse[binned_bitmask_offset + 2u * gl_WorkGroupID.x + 1u] = ballot_result.y;
        }
        else if (gl_SubgroupSize == 32u)
        {
            binned_bitmask_coarse[binned_bitmask_offset + gl_SubgroupID + (gl_WorkGroupSize.x / 32u) * gl_WorkGroupID.x] = ballot_result.x;
        }
    }
#else
    if (local_index == 0u)
        merged_mask = 0u;
    barrier();

    if (group_bin_to_tile)
        atomicOr(merged_mask, 1u << local_index);

    barrier();

    if (local_index == 0u)
    {
        uint binned_bitmask_offset = uint(linear_tile);
        binned_bitmask_coarse[binned_bitmask_offset + gl_WorkGroupID.x] = merged_mask;
    }

#if !UBERSHADER
    uint bit_count = uint(bitCount(binned));
    uint instance_offset = 0u;
    if (bit_count != 0u)
        instance_offset = atomicAdd(indirect_counts.elems[0].w, bit_count);
#endif
#endif

#if !UBERSHADER
    // Distribute shading work.
    if (bit_count != 0u)
        tile_instance_offsets.elems[linear_tile * TILE_BINNING_STRIDE + mask_index] = instance_offset;

    while (binned != 0u)
    {
        int i = findLSB(binned);
        binned &= ~uint(1 << i);
        int primitive_index = i + mask_index * 32;
        uint variant_index = uint(state_indices.elems[primitive_index].static_depth_tmem.x);

        uint work_offset = allocate_work_offset(variant_index);
        tile_raster_work.elems[work_offset + uint(TILE_INSTANCE_STRIDE) * variant_index] =
            uvec4(tile.x, tile.y, instance_offset, primitive_index);
        instance_offset++;
    }
#endif
}